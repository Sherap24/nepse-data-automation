name: NEPSE Data Collection + Drive Uploads

on:
  schedule:
    # NEPSE Trading Hours (NPT=UTC+5:45)
    # Sunday-Thursday full day (11:00–15:00 NPT)
    - cron: '15 5 * * 0,1,2,3,6'
    - cron: '30 5 * * 0,1,2,3,6'
    - cron: '45 5 * * 0,1,2,3,6'
    - cron: '0 6 * * 0,1,2,3,6'
    - cron: '15 6 * * 0,1,2,3,6'
    - cron: '30 6 * * 0,1,2,3,6'
    - cron: '45 6 * * 0,1,2,3,6'
    - cron: '0 7 * * 0,1,2,3,6'
    - cron: '15 7 * * 0,1,2,3,6'
    - cron: '30 7 * * 0,1,2,3,6'
    - cron: '45 7 * * 0,1,2,3,6'
    - cron: '0 8 * * 0,1,2,3,6'
    - cron: '15 8 * * 0,1,2,3,6'
    - cron: '30 8 * * 0,1,2,3,6'
    - cron: '45 8 * * 0,1,2,3,6'
    - cron: '0 9 * * 0,1,2,3,6'
    - cron: '15 9 * * 0,1,2,3,6'
    # Friday short day (11:00–13:00 NPT)
    - cron: '15 5 * * 5'
    - cron: '30 5 * * 5'
    - cron: '45 5 * * 5'
    - cron: '0 6 * * 5'
    - cron: '15 6 * * 5'
    - cron: '30 6 * * 5'
    - cron: '45 6 * * 5'
    - cron: '0 7 * * 5'
    - cron: '15 7 * * 5'
  workflow_dispatch:

permissions:
  contents: write

jobs:
  collect-nepse-data:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install Python deps
        run: |
          pip install requests pandas pytz schedule

      - name: Set up NepseAPI Server
        run: |
          git clone https://github.com/surajrimal07/NepseAPI.git
          cd NepseAPI
          pip install -r requirements.txt
          nohup python server.py &
          sleep 30

      - name: Test API Connection
        run: |
          for i in {1..5}; do
            if curl -f http://localhost:8000/ > /dev/null 2>&1; then
              echo "API server is running!"
              break
            else
              echo "Attempt $i/5: API not ready, waiting 10 seconds..."
              sleep 10
            fi
          done
          curl -f http://localhost:8000/ || (echo "API server failed to start" && exit 1)

      - name: Run NEPSE Data Collection
        run: |
          python -c "
          import sys
          sys.path.append('.')
          from cloud_collector import CloudNepseCollector
          import datetime, pytz
          nepal_tz = pytz.timezone('Asia/Kathmandu')
          now = datetime.datetime.now(nepal_tz)
          collector = CloudNepseCollector()
          print(f'Current Nepal time: {now}')
          print(f'Day: {now.strftime(\"%A\")} (weekday {now.weekday()})')
          print(f'Time: {now.strftime(\"%H:%M NPT\")}')
          print(f'Market schedule: {collector.get_market_schedule_info(now)}')
          if collector.is_market_open(now):
              print('Market should be open - collecting data...')
              result = collector.collect_single_run()
              if result == 'market_closed':
                  print('Market detected as closed during collection')
              elif result:
                  print(f'Data collection successful: {result}')
              else:
                  print('Data collection failed')
          else:
              print('Market is closed - skipping collection')
              print('Next trading: Check schedule for market hours')
          "

      - name: Show collection results
        if: always()
        run: |
          echo "Collection Results:"
          if [ -d "data" ]; then
            echo "Files created: $(ls data/ | wc -l)"
            ls -lh data/
            if ls data/*summary*.json 1> /dev/null 2>&1; then
              echo "Latest collection summary:"
              cat data/*summary*.json | tail -1 | python -m json.tool || echo "Summary not readable"
            fi
          else
            echo "No data directory found"
          fi
          if [ -f "logs/cloud_collector.log" ]; then
            echo "Recent collector logs:"
            tail -10 logs/cloud_collector.log
          fi

      - name: Upload artifact (runner)
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: nepse-data-${{ github.run_number }}
          path: |
            data/
            logs/
          retention-days: 30

      - name: Commit and push data (if any)
        if: always()
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          if [ -d "data" ] && [ "$(ls -A data)" ]; then
            git add data/ logs/ || true
            git commit -m "Automated NEPSE data collection - $(date -u)" || true
            git push || true
            echo "Data committed to repository"
          else
            echo "No data files to commit"
          fi

      # ===== Google Drive upload & housekeeping =====
      - name: Install rclone
        run: |
          curl -fsSL https://rclone.org/install.sh | sudo bash

      - name: Configure rclone (OAuth to My Drive)
        run: |
          mkdir -p ~/.config/rclone
          cat > ~/.config/rclone/rclone.conf <<'EOF'
          ${{ secrets.RCLONE_CONF }}
          EOF
          rclone --version

      - name: Compress CSVs on runner (saves bandwidth + Drive space)
        run: |
          if [ -d "data" ]; then
            mkdir -p tmp_compressed
            find data -type f -name '*.csv' -print0 | while IFS= read -r -d '' f; do
              gzip -c "$f" > "tmp_compressed/$(basename "$f").gz"
            done
          fi

      - name: Upload data & logs to Drive (date-partitioned)
        run: |
          DEST_DATE=$(date -u +'%Y/%m/%d')

          # 1) Upload compressed CSVs
          if [ -d "tmp_compressed" ] && [ "$(ls -A tmp_compressed)" ]; then
            rclone copy "tmp_compressed" "gdrive:${DEST_DATE}/raw" \
              --fast-list --transfers=8 --checkers=16 --create-empty-src-dirs \
              --drive-stop-on-upload-limit
          fi

          # 2) Upload any non-CSV data files (binaries, json, etc.)
          if [ -d "data" ]; then
            find data -type f ! -name '*.csv' -print0 | while IFS= read -r -d '' f; do
              rclone copy "$f" "gdrive:${DEST_DATE}/raw" --fast-list --transfers=4 --checkers=8 || true
            done
          fi

          # 3) Upload logs
          if [ -d "logs" ] && [ "$(ls -A logs)" ]; then
            rclone copy "logs" "gdrive:${DEST_DATE}/logs" \
              --fast-list --transfers=4 --checkers=8 --create-empty-src-dirs \
              --drive-stop-on-upload-limit
          fi

      - name: Remote retention (prune old raw >120d; keep snapshots)
        run: |
          rclone delete gdrive: --min-age 120d --include "*/raw/**" --drive-use-trash || true
          rclone rmdirs gdrive: --leave-root || true

      # ---------- Fallback: if no new data, sweep repo CSVs and upload (OAuth) ----------
      - name: Fallback upload — sweep repo CSVs with rclone
        if: always()
        run: |
          if [ ! -d "data" ] || [ -z "$(ls -A data 2>/dev/null)" ]; then
            echo "No new data files; uploading any *.csv in the repo."
            DEST_DATE=$(date -u +'%Y/%m/%d')
            mkdir -p tmp_repo_gz
            find . -type f -name '*.csv' -print0 | while IFS= read -r -d '' f; do
              gzip -c "$f" > "tmp_repo_gz/$(basename "$f").gz"
            done
            rclone copy "tmp_repo_gz" "gdrive:${DEST_DATE}/repo-csvs" \
              --fast-list --transfers=8 --checkers=16 --create-empty-src-dirs
          else
            echo "New data exists; skipping repo fallback."
          fi

  # Manual backfill: download ALL historical artifacts, then upload to My Drive (OAuth)
  backfill-csvs-to-drive:
    if: github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download ALL artifacts from history
        uses: dawidd6/action-download-artifact@v3
        with:
          workflow_conclusion: completed
          skip_unpack: false
          path: _artifacts

      - name: Install rclone
        run: |
          curl -fsSL https://rclone.org/install.sh | sudo bash

      - name: Configure rclone (OAuth to My Drive)
        run: |
          mkdir -p ~/.config/rclone
          cat > ~/.config/rclone/rclone.conf <<'EOF'
          ${{ secrets.RCLONE_CONF }}
          EOF
          rclone --version

      - name: Compress & upload all artifact CSVs
        run: |
          mkdir -p .artifact_csvs_gz
          find _artifacts -type f -name '*.csv' -print0 | while IFS= read -r -d '' f; do
            gzip -c "$f" > ".artifact_csvs_gz/$(basename "$f").gz"
          done
          DEST_DATE=$(date -u +'%Y/%m/%d')
          rclone copy ".artifact_csvs_gz" "gdrive:${DEST_DATE}/backfill-artifacts" \
            --fast-list --transfers=8 --checkers=16 --create-empty-src-dirs

      - name: Also sweep repo CSVs (historical files in repo)
        run: |
          mkdir -p .repo_csvs_gz
          find . -type f -name '*.csv' -print0 | while IFS= read -r -d '' f; do
            gzip -c "$f" > ".repo_csvs_gz/$(basename "$f").gz"
          done
          DEST_DATE=$(date -u +'%Y/%m/%d')
          rclone copy ".repo_csvs_gz" "gdrive:${DEST_DATE}/backfill-repo" \
            --fast-list --transfers=8 --checkers=16 --create-empty-src-dirs
