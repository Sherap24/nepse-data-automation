name: NEPSE Data Collection + Drive Uploads

on:
  schedule:
    # NEPSE Trading Hours (NPT=UTC+5:45)
    # Sunday-Thursday full day (11:00–15:00 NPT)
    - cron: '15 5 * * 0,1,2,3,6'
    - cron: '30 5 * * 0,1,2,3,6'
    - cron: '45 5 * * 0,1,2,3,6'
    - cron: '0 6 * * 0,1,2,3,6'
    - cron: '15 6 * * 0,1,2,3,6'
    - cron: '30 6 * * 0,1,2,3,6'
    - cron: '45 6 * * 0,1,2,3,6'
    - cron: '0 7 * * 0,1,2,3,6'
    - cron: '15 7 * * 0,1,2,3,6'
    - cron: '30 7 * * 0,1,2,3,6'
    - cron: '45 7 * * 0,1,2,3,6'
    - cron: '0 8 * * 0,1,2,3,6'
    - cron: '15 8 * * 0,1,2,3,6'
    - cron: '30 8 * * 0,1,2,3,6'
    - cron: '45 8 * * 0,1,2,3,6'
    - cron: '0 9 * * 0,1,2,3,6'
    - cron: '15 9 * * 0,1,2,3,6'
    # Friday short day (11:00–13:00 NPT)
    - cron: '15 5 * * 5'
    - cron: '30 5 * * 5'
    - cron: '45 5 * * 5'
    - cron: '0 6 * * 5'
    - cron: '15 6 * * 5'
    - cron: '30 6 * * 5'
    - cron: '45 6 * * 5'
    - cron: '0 7 * * 5'
    - cron: '15 7 * * 5'
  workflow_dispatch:

permissions:
  contents: write

jobs:
  collect-nepse-data:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install Python deps
        run: |
          pip install requests pandas pytz schedule

      - name: Set up NepseAPI Server
        run: |
          git clone https://github.com/surajrimal07/NepseAPI.git
          cd NepseAPI
          pip install -r requirements.txt
          nohup python server.py &
          sleep 30

      - name: Test API Connection
        run: |
          for i in {1..5}; do
            if curl -f http://localhost:8000/ > /dev/null 2>&1; then
              echo "API server is running!"
              break
            else
              echo "Attempt $i/5: API not ready, waiting 10 seconds..."
              sleep 10
            fi
          done
          curl -f http://localhost:8000/ || (echo "API server failed to start" && exit 1)

      - name: Run NEPSE Data Collection
        run: |
          python -c "
          import sys
          sys.path.append('.')
          from cloud_collector import CloudNepseCollector
          import datetime, pytz
          nepal_tz = pytz.timezone('Asia/Kathmandu')
          now = datetime.datetime.now(nepal_tz)
          collector = CloudNepseCollector()
          print(f'Current Nepal time: {now}')
          print(f'Day: {now.strftime(\"%A\")} (weekday {now.weekday()})')
          print(f'Time: {now.strftime(\"%H:%M NPT\")}')
          print(f'Market schedule: {collector.get_market_schedule_info(now)}')
          if collector.is_market_open(now):
              print('Market should be open - collecting data...')
              result = collector.collect_single_run()
              if result == 'market_closed':
                  print('Market detected as closed during collection')
              elif result:
                  print(f'Data collection successful: {result}')
              else:
                  print('Data collection failed')
          else:
              print('Market is closed - skipping collection')
              print('Next trading: Check schedule for market hours')
          "

      - name: Show collection results
        if: always()
        run: |
          echo "Collection Results:"
          if [ -d "data" ]; then
            echo "Files created: $(ls data/ | wc -l)"
            ls -lh data/
            if ls data/*summary*.json 1> /dev/null 2>&1; then
              echo "Latest collection summary:"
              cat data/*summary*.json | tail -1 | python -m json.tool || echo "Summary not readable"
            fi
          else
            echo "No data directory found"
          fi
          if [ -f "logs/cloud_collector.log" ]; then
            echo "Recent collector logs:"
            tail -10 logs/cloud_collector.log
          fi

      - name: Upload artifact (runner)
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: nepse-data-${{ github.run_number }}
          path: |
            data/
            logs/
          retention-days: 30

      - name: Commit and push data (if any)
        if: always()
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          if [ -d "data" ] && [ "$(ls -A data)" ]; then
            git add data/ logs/ || true
            git commit -m "Automated NEPSE data collection - $(date -u)" || true
            git push || true
            echo "Data committed to repository"
          else
            echo "No data files to commit"
          fi

      # ===== Google Drive upload & housekeeping =====
      - name: Install rclone
        run: |
          curl -fsSL https://rclone.org/install.sh | sudo bash

      - name: Configure rclone (OAuth to My Drive)
        run: |
          mkdir -p ~/.config/rclone
          cat > ~/.config/rclone/rclone.conf <<'EOF'
          ${{ secrets.RCLONE_CONF }}
          EOF
          rclone --version

      - name: Compress CSVs on runner (saves bandwidth + Drive space)
        run: |
          if [ -d "data" ]; then
            mkdir -p tmp_compressed
            find data -type f -name '*.csv' -print0 | while IFS= read -r -d '' f; do
              gzip -c "$f" > "tmp_compressed/$(basename "$f").gz"
            done
          fi

      - name: Upload data & logs to Drive (date-partitioned)
        run: |
          DEST_DATE=$(date -u +'%Y/%m/%d')

          # 1) Upload compressed CSVs
          if [ -d "tmp_compressed" ] && [ "$(ls -A tmp_compressed)" ]; then
            rclone copy "tmp_compressed" "gdrive:${DEST_DATE}/raw" \
              --fast-list --transfers=8 --checkers=16 --create-empty-src-dirs \
              --drive-stop-on-upload-limit
          fi

          # 2) Upload any non-CSV data files (binaries, json, etc.)
          if [ -d "data" ]; then
            find data -type f ! -name '*.csv' -print0 | while IFS= read -r -d '' f; do
              rclone copy "$f" "gdrive:${DEST_DATE}/raw" --fast-list --transfers=4 --checkers=8 || true
            done
          fi

          # 3) Upload logs
          if [ -d "logs" ] && [ "$(ls -A logs)" ]; then
            rclone copy "logs" "gdrive:${DEST_DATE}/logs" \
              --fast-list --transfers=4 --checkers=8 --create-empty-src-dirs \
              --drive-stop-on-upload-limit
          fi

      - name: Remote retention (prune old raw >120d; keep snapshots)
        run: |
          rclone delete gdrive: --min-age 120d --include "*/raw/**" --drive-use-trash || true
          rclone rmdirs gdrive: --leave-root || true

      # ---------- Fallback: if no new data, sweep repo CSVs and upload (OAuth) ----------
      - name: Fallback upload — sweep repo CSVs with rclone
        if: always()
        run: |
          if [ ! -d "data" ] || [ -z "$(ls -A data 2>/dev/null)" ]; then
            echo "No new data files; uploading any *.csv in the repo."
            DEST_DATE=$(date -u +'%Y/%m/%d')
            mkdir -p tmp_repo_gz
            find . -type f -name '*.csv' -print0 | while IFS= read -r -d '' f; do
              gzip -c "$f" > "tmp_repo_gz/$(basename "$f").gz"
            done
            rclone copy "tmp_repo_gz" "gdrive:${DEST_DATE}/repo-csvs" \
              --fast-list --transfers=8 --checkers=16 --create-empty-src-dirs
          else
            echo "New data exists; skipping repo fallback."
          fi

  # Manual backfill: download ALL historical artifacts, sort by original date, then upload to My Drive
  backfill-csvs-to-drive:
    if: github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download ALL artifacts from history
        uses: dawidd6/action-download-artifact@v3
        with:
          workflow_conclusion: completed
          skip_unpack: false
          path: _artifacts

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install rclone
        run: |
          curl -fsSL https://rclone.org/install.sh | sudo bash

      - name: Configure rclone (OAuth to My Drive)
        run: |
          mkdir -p ~/.config/rclone
          cat > ~/.config/rclone/rclone.conf <<'EOF'
          ${{ secrets.RCLONE_CONF }}
          EOF
          rclone --version

      - name: Backfill — partition artifact CSVs by detected date and upload
        run: |
          python - <<'PY'
          import os, re, gzip, pathlib, datetime, shutil, sys
          SRC = "_artifacts"
          STAGE = ".backfill_stage"  # local tree: YYYY/MM/DD/backfill-artifacts
          os.makedirs(STAGE, exist_ok=True)

          PATS = [
              re.compile(r"(20\\d{2})(\\d{2})(\\d{2})"),          # 20250819
              re.compile(r"(20\\d{2})[-_](\\d{2})[-_](\\d{2})"),  # 2025-08-19 or 2025_08_19
          ]

          def guess_date(path):
              name = os.path.basename(path)
              for pat in PATS:
                  m = pat.search(name)
                  if m:
                      return m.group(1), m.group(2), m.group(3)
              p = pathlib.Path(path)
              for part in p.parts[::-1]:
                  for pat in PATS:
                      m = pat.search(part)
                      if m:
                          return m.group(1), m.group(2), m.group(3)
              ts = os.path.getmtime(path)
              dt = datetime.datetime.utcfromtimestamp(ts).date()
              return f"{dt.year:04d}", f"{dt.month:02d}", f"{dt.day:02d}"

          def gz_copy(src_path, dst_gz):
              os.makedirs(os.path.dirname(dst_gz), exist_ok=True)
              with open(src_path, "rb") as fin, gzip.open(dst_gz, "wb") as fout:
                  shutil.copyfileobj(fin, fout)

          csv_count = 0
          for root, _, files in os.walk(SRC):
              for fn in files:
                  if not fn.lower().endswith(".csv"):
                      continue
                  src = os.path.join(root, fn)
                  y, m, d = guess_date(src)
                  dest_dir = os.path.join(STAGE, y, m, d, "backfill-artifacts")
                  dest_gz = os.path.join(dest_dir, f"{fn}.gz")
                  gz_copy(src, dest_gz)
                  csv_count += 1
                  if csv_count % 50 == 0:
                      print(f"Staged {csv_count} CSVs so far...")

          print(f"Staged total {csv_count} CSVs." if csv_count else "No CSVs found in artifacts.")
          PY

          # Mirror the local YYYY/MM/DD/backfill-artifacts tree to Drive root
          rclone copy ".backfill_stage" "gdrive:" \
            --fast-list --transfers=8 --checkers=16 --create-empty-src-dirs

      - name: Also sweep repo CSVs (historical files in repo, partition by date)
        run: |
          python - <<'PY'
          import os, re, gzip, pathlib, datetime, shutil, sys
          SRC = "."
          STAGE = ".repo_backfill_stage"
          EXCLUDE_DIRS = {".git", ".github", ".venv", "venv", "env", "__pycache__", "build", "dist",
                          ".mypy_cache", ".pytest_cache", ".ruff_cache", "_artifacts"}
          os.makedirs(STAGE, exist_ok=True)

          PATS = [
              re.compile(r"(20\\d{2})(\\d{2})(\\d{2})"),
              re.compile(r"(20\\d{2})[-_](\\d{2})[-_](\\d{2})"),
          ]

          def guess_date(path):
              name = os.path.basename(path)
              for pat in PATS:
                  m = pat.search(name)
                  if m:
                      return m.group(1), m.group(2), m.group(3)
              p = pathlib.Path(path)
              for part in p.parts[::-1]:
                  for pat in PATS:
                      m = pat.search(part)
                      if m:
                          return m.group(1), m.group(2), m.group(3)
              ts = os.path.getmtime(path)
              dt = datetime.datetime.utcfromtimestamp(ts).date()
              return f"{dt.year:04d}", f"{dt.month:02d}", f"{dt.day:02d}"

          def gz_copy(src_path, dst_gz):
              os.makedirs(os.path.dirname(dst_gz), exist_ok=True)
              with open(src_path, "rb") as fin, gzip.open(dst_gz, "wb") as fout:
                  shutil.copyfileobj(fin, fout)

          csv_count = 0
          for root, dirs, files in os.walk(SRC, topdown=True):
              dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]
              for fn in files:
                  if not fn.lower().endswith(".csv"):
                      continue
                  src = os.path.join(root, fn)
                  y, m, d = guess_date(src)
                  dest_dir = os.path.join(STAGE, y, m, d, "backfill-repo")
                  dest_gz = os.path.join(dest_dir, f"{fn}.gz")
                  gz_copy(src, dest_gz)
                  csv_count += 1

          print(f"Staged total {csv_count} repo CSVs." if csv_count else "No CSVs found in repo.")
          PY

          rclone copy ".repo_backfill_stage" "gdrive:" \
            --fast-list --transfers=8 --checkers=16 --create-empty-src-dirs
